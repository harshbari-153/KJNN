{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Harsh Bari\n",
        "## bari.harsh2001@gmail.com\n",
        "## Gujarat, India"
      ],
      "metadata": {
        "id": "zZdUf5J4GT_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Header Files"
      ],
      "metadata": {
        "id": "5WpYqqnjEmB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "import math\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.downloader import load\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import gc\n",
        "#from gensim.downloader import api"
      ],
      "metadata": {
        "id": "NLkA88k8EgiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD7RIW-YIbhn",
        "outputId": "71760dac-5ec4-4ee7-f09c-f74ad541ac82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f2815939ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enable GPU"
      ],
      "metadata": {
        "id": "6xzT6yUYLKSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"System Running on \" + str(dev))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e4zZGfQLR2j",
        "outputId": "b8e180a1-ef04-496a-d150-67def3e88efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System Running on cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Word Embeddings"
      ],
      "metadata": {
        "id": "EI3DK8VTJysS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  with open('wiki_model.bin', 'r') as f:\n",
        "    w_model = gensim.models.KeyedVectors.load('wiki_model.bin')\n",
        "except FileNotFoundError:\n",
        "  wiki_model = gensim.downloader.load('glove-wiki-gigaword-100')\n",
        "  #wiki_model = api.load('glove-wiki-gigaword-100')\n",
        "  wiki_model.save('wiki_model.bin')\n",
        "  w_model = gensim.models.KeyedVectors.load('wiki_model.bin')"
      ],
      "metadata": {
        "id": "OyhKdGKHGf9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d0dc034-26da-4293-eb66-76e501ce9d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_embeddings(word):\n",
        "  word = word.lower()\n",
        "  if word in w_model.key_to_index:\n",
        "    return torch.tensor(w_model[word], device = dev)\n",
        "  return torch.tensor(w_model[\"default\"], device = dev)"
      ],
      "metadata": {
        "id": "hyQyctCSJKZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process Sentence"
      ],
      "metadata": {
        "id": "7YLrFS6DpjEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = [\"a\", \"an\", \"the\", \"of\", \"in\", \"for\", \"through\", \"there\", \"be\", \"is\", \"was\", \"will\", \"and\", \"or\", \"not\", \"no\", \"on\", \"at\", \"under\", \"such\", \"that\", \"to\", \"with\"]\n",
        "punctuation_marks = \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
        "\n",
        "def remove_punctuation(sentence):\n",
        "  return ''.join(char for char in sentence if char not in punctuation_marks)\n",
        "\n",
        "\n",
        "def process_sentence(sentence):\n",
        "\n",
        "  # lowering cases\n",
        "  sentence = sentence.lower()\n",
        "\n",
        "  # remove punctuations\n",
        "  sentence = remove_punctuation(sentence)\n",
        "\n",
        "  # tokenize\n",
        "  tokens = sentence.split()\n",
        "\n",
        "  # remove stop words\n",
        "  # tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "  # Get embeddings for each token\n",
        "  embeddings = [get_word_embeddings(token) for token in tokens]\n",
        "\n",
        "  return torch.stack(embeddings)"
      ],
      "metadata": {
        "id": "z5HCfFsPpic8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Sentence Subject"
      ],
      "metadata": {
        "id": "WC0tmnrj81tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_sentence_subject(sentence):\n",
        "\n",
        "  # lowering cases\n",
        "  sentence = sentence.lower()\n",
        "\n",
        "  # remove punctuations\n",
        "  sentence = remove_punctuation(sentence)\n",
        "\n",
        "  # apply pos tagging\n",
        "  doc = nlp(sentence)\n",
        "\n",
        "  # test\n",
        "  # for token in doc:\n",
        "  #   print(f\"Token: {token.text}, POS: {token.pos_}\")\n",
        "\n",
        "  for token in doc:\n",
        "    if str(token.pos_)[-1] == \"N\":\n",
        "      return token.text\n",
        "\n",
        "  return doc[0].text"
      ],
      "metadata": {
        "id": "qXlHFaS28tbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_chunks(vectors):\n",
        "  return len(vectors)"
      ],
      "metadata": {
        "id": "owgF_5jVLKU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encodding"
      ],
      "metadata": {
        "id": "jCuCCX6FP876"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_positional_encodding(sequence):\n",
        "  n, d_model = sequence.shape\n",
        "  assert d_model == 100, \"Input tensor must have 100 features (shape: [n, 100])\"\n",
        "\n",
        "  # Create the positional encoding matrix\n",
        "  position = torch.arange(n, dtype=torch.float).unsqueeze(1)  # Shape: (n, 1)\n",
        "  div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "  pe = torch.zeros((n, d_model), device = dev)  # Initialize positional encoding tensor\n",
        "  pe[:, 0::2] = torch.sin(position * div_term)  # sin for even indices\n",
        "  pe[:, 1::2] = torch.cos(position * div_term)  # cos for odd indices\n",
        "\n",
        "  # Add positional encoding to input tensor\n",
        "  output_tensor = sequence + pe\n",
        "  return output_tensor"
      ],
      "metadata": {
        "id": "lHhkUgNr5szt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine Claim and Justification"
      ],
      "metadata": {
        "id": "C7BADP2FTV74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine(C, J):\n",
        "\n",
        "  n = C.shape[0]\n",
        "\n",
        "  C_flat = C.view(-1)\n",
        "  J_flat = J.view(-1)\n",
        "\n",
        "  new_vector = torch.empty(2*n, dtype = C.dtype)\n",
        "  new_vector[0::2] = C_flat\n",
        "  new_vector[1::2] = J_flat\n",
        "\n",
        "  return new_vector.view(-1, 1)"
      ],
      "metadata": {
        "id": "QISi3s34gFNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add And Normalize"
      ],
      "metadata": {
        "id": "kc5ROE29U-bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_and_normalize(sequence):\n",
        "  sum_vec = sum(sequence)\n",
        "\n",
        "  norm = torch.norm(sum_vec)\n",
        "\n",
        "  if norm == 0:\n",
        "    norm = 1\n",
        "\n",
        "  return sum_vec / norm"
      ],
      "metadata": {
        "id": "lAqJ62E0U9sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train QKV Matrices"
      ],
      "metadata": {
        "id": "hUNfrI_dR5vF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunked_avgerage(chunks, inputs):\n",
        "  assert sum(chunks) == inputs.size(0), \"The sum of chunks must equal the first dimension of inputs.\"\n",
        "\n",
        "  output = []\n",
        "  start_idx = 0\n",
        "\n",
        "  for chunk_size in chunks:\n",
        "    end_idx = start_idx + chunk_size\n",
        "    chunk_avg = inputs[start_idx:end_idx].mean(dim=0)\n",
        "    output.append(chunk_avg)\n",
        "    start_idx = end_idx\n",
        "\n",
        "  return torch.tensor(torch.stack(output))"
      ],
      "metadata": {
        "id": "xD_Bo2jSyq8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_QKV(inputs, targets, chunks, query, key, value, num_epochs, learning_rate):\n",
        "\n",
        "  # Adam Optimizer\n",
        "  optimizer = optim.Adam([query, key, value], lr=learning_rate)\n",
        "\n",
        "  # Mean Squared Error\n",
        "  loss_fn = nn.MSELoss()\n",
        "  final_loss = 0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Compute Q, K, V\n",
        "    Q = inputs @ query\n",
        "    K = inputs @ key\n",
        "    V = inputs @ value\n",
        "\n",
        "    # Attention scores (Q.K^T) and softmax normalization\n",
        "    attention_scores = Q @ K.T\n",
        "    attention_weights = torch.softmax(attention_scores, dim=1)\n",
        "\n",
        "    # Weighted sum of values\n",
        "    output2 = attention_weights @ V\n",
        "    #output = chunked_avgerage(chunks, output2)\n",
        "\n",
        "    output = []\n",
        "    start = 0\n",
        "    for c in chunks:\n",
        "        output.append(output2[start:start + c].mean(dim=0))\n",
        "        start += c\n",
        "\n",
        "\n",
        "    output = torch.stack(output)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = loss_fn(output, targets)\n",
        "\n",
        "    # Backpropagation and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "    final_loss += epoch_loss\n",
        "\n",
        "    #print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "  print(\"Total Loss: \" + str(final_loss/num_epochs))\n",
        "  return query, key, value"
      ],
      "metadata": {
        "id": "7QPI08ZXn4jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Self Attention Embeddings from Query, Key and Value"
      ],
      "metadata": {
        "id": "3GxWCrRtSsPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_self_attention(query, key, value, vectors):\n",
        "\n",
        "  new_vectors = []\n",
        "\n",
        "  for vec in vectors:\n",
        "    q = vec @ query\n",
        "    k = vec @ key\n",
        "    v = vec @ value\n",
        "\n",
        "    output = (q @ k.T) @ V\n",
        "    new_vectors.append(output)\n",
        "\n",
        "  return torch.stack(new_vectors)"
      ],
      "metadata": {
        "id": "oT1ZFa2TYlQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seralize Input"
      ],
      "metadata": {
        "id": "lx41lZkmNM9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seralize_input(input_vectors):\n",
        "  return torch.cat(input_vectors, dim=0)"
      ],
      "metadata": {
        "id": "uAROfou8NKgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Claim QKV Matrices"
      ],
      "metadata": {
        "id": "PthZsRpF0tmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Claims\n",
        "\n",
        "dataset = pd.read_json('/content/politifact_factcheck_data.json', lines = True)\n",
        "dataset['statement'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "d8ttD4Gd8Clj",
        "outputId": "839eaa9b-ebe6-40c5-d93b-6c6bdce3c2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    John McCain opposed bankruptcy protections for...\n",
              "1    \"Bennie Thompson actively cheer-led riots in t...\n",
              "2    Says Maggie Hassan was \"out of state on 30 day...\n",
              "3    \"BUSTED: CDC Inflated COVID Numbers, Accused o...\n",
              "4    \"I'm the only (Republican) candidate that has ...\n",
              "Name: statement, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>John McCain opposed bankruptcy protections for...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"Bennie Thompson actively cheer-led riots in t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Says Maggie Hassan was \"out of state on 30 day...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"BUSTED: CDC Inflated COVID Numbers, Accused o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"I'm the only (Republican) candidate that has ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "\n",
        "c_query = torch.randn(100, 100, device=dev, requires_grad=True)\n",
        "c_key = torch.randn(100, 100, device=dev, requires_grad=True)\n",
        "c_value = torch.randn(100, 100, device=dev, requires_grad=True)\n",
        "\n",
        "c_learning_rate = 0.001\n",
        "c_epoches = 20\n",
        "\n",
        "chunks = 1000\n",
        "total_claims = 21146\n",
        "\n",
        "start = 0\n",
        "while start < total_claims:\n",
        "  print(\"Processing \" + str(round(start*100/total_claims, 2)) + \"% of the chunk\")\n",
        "  if start == 21000:\n",
        "    end = total_claims\n",
        "  else:\n",
        "    end = start + chunks\n",
        "\n",
        "  c_inputs = list(map(process_sentence, dataset.iloc[start:end]['statement']))\n",
        "  c_targets = list(map(get_word_embeddings, list(map(get_sentence_subject, dataset.iloc[start:end]['statement']))))\n",
        "  c_chunks = torch.tensor(list(map(create_chunks, c_inputs)))\n",
        "\n",
        "  c_query, c_key, c_value = train_QKV(seralize_input(c_inputs), torch.stack(c_targets), c_chunks, c_query, c_key, c_value, c_epoches, c_learning_rate)\n",
        "\n",
        "  start += chunks\n",
        "  print(\"\")\n",
        "\n",
        "# turn off gradients\n",
        "c_query.requires_grad_(False)\n",
        "c_key.requires_grad_(False)\n",
        "c_value.requires_grad_(False)\n",
        "\n",
        "# save matrices\n",
        "torch.save(c_query, \"c_query.pt\")\n",
        "torch.save(c_key, \"c_key.pt\")\n",
        "torch.save(c_value, \"c_value.pt\")\n",
        "\n",
        "# to load them\n",
        "# c_query = torch.load(\"c_query.pt\")\n",
        "\n",
        "# download\n",
        "files.download(\"c_query.pt\")\n",
        "files.download(\"c_key.pt\")\n",
        "files.download(\"c_value.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PWfTgp7gebKf",
        "outputId": "698412eb-32fc-4455-8e80-6af3b0bbc898",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Chunk of 0.0\n",
            "Total Loss: 6.960228514671326\n",
            "\n",
            "Processing Chunk of 4.729026766291497\n",
            "Total Loss: 4.918579530715943\n",
            "\n",
            "Processing Chunk of 9.458053532582994\n",
            "Total Loss: 6.759651064872742\n",
            "\n",
            "Processing Chunk of 14.187080298874491\n",
            "Total Loss: 4.5226975440979\n",
            "\n",
            "Processing Chunk of 18.91610706516599\n",
            "Total Loss: 3.881887376308441\n",
            "\n",
            "Processing Chunk of 23.645133831457485\n",
            "Total Loss: 4.375174868106842\n",
            "\n",
            "Processing Chunk of 28.374160597748983\n",
            "Total Loss: 3.028776156902313\n",
            "\n",
            "Processing Chunk of 33.10318736404048\n",
            "Total Loss: 3.050463926792145\n",
            "\n",
            "Processing Chunk of 37.83221413033198\n",
            "Total Loss: 4.188982748985291\n",
            "\n",
            "Processing Chunk of 42.561240896623474\n",
            "Total Loss: 2.9442052483558654\n",
            "\n",
            "Processing Chunk of 47.29026766291497\n",
            "Total Loss: 3.3247101664543153\n",
            "\n",
            "Processing Chunk of 52.01929442920647\n",
            "Total Loss: 2.465067946910858\n",
            "\n",
            "Processing Chunk of 56.748321195497965\n",
            "Total Loss: 2.6472737193107605\n",
            "\n",
            "Processing Chunk of 61.47734796178946\n",
            "Total Loss: 2.3507472038269044\n",
            "\n",
            "Processing Chunk of 66.20637472808096\n",
            "Total Loss: 2.273613393306732\n",
            "\n",
            "Processing Chunk of 70.93540149437246\n",
            "Total Loss: 2.386524331569672\n",
            "\n",
            "Processing Chunk of 75.66442826066395\n",
            "Total Loss: 2.101821428537369\n",
            "\n",
            "Processing Chunk of 80.39345502695545\n",
            "Total Loss: 2.2425565004348753\n",
            "\n",
            "Processing Chunk of 85.12248179324695\n",
            "Total Loss: 2.2603443384170534\n",
            "\n",
            "Processing Chunk of 89.85150855953844\n",
            "Total Loss: 2.1424951195716857\n",
            "\n",
            "Processing Chunk of 94.58053532582994\n",
            "Total Loss: 2.0787285923957826\n",
            "\n",
            "Processing Chunk of 99.30956209212144\n",
            "Total Loss: 2.0737613797187806\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c15c61ee-255b-4a21-97de-ab4991b59b9a\", \"c_query.pt\", 41180)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0d1ce843-10ee-4a21-860d-7c69f9af7644\", \"c_key.pt\", 41106)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ad2757a6-2716-4997-b2bc-21c02d6e7985\", \"c_value.pt\", 41180)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Justification QKV Matrices"
      ],
      "metadata": {
        "id": "EylsAFnK6moT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Justifications\n",
        "\n",
        "with zipfile.ZipFile(\"/content/top_justifications.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")"
      ],
      "metadata": {
        "id": "X-ZtLVK4WQCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Files\n",
        "\n",
        "def get_justifications(start, end):\n",
        "  sentences = []\n",
        "\n",
        "  for i in range(start, end):\n",
        "    file_path = f\"/content/top_justifications/top_justification_{i}.txt\"\n",
        "\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "      lines = file.readlines()\n",
        "\n",
        "    sentences.extend(lines[1:])\n",
        "\n",
        "  df = pd.DataFrame(sentences, columns=[\"sentence\"])\n",
        "\n",
        "  # Clean Whitespace and Newlines\n",
        "  df[\"sentence\"] = df[\"sentence\"].str.strip()\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "y7LtlrqjcG_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "\n",
        "j_query = torch.randn(100, 100, device=dev, requires_grad=True)\n",
        "j_key = torch.randn(100, 100, device=dev, requires_grad=True)\n",
        "j_value = torch.randn(100, 100, device=dev, requires_grad=True)\n",
        "\n",
        "j_learning_rate = 0.001\n",
        "j_epoches = 15\n",
        "\n",
        "chunks = 200\n",
        "total_claims = 21146\n",
        "\n",
        "start = 0\n",
        "while start < total_claims:\n",
        "  print(\"Processing \" + str(round(start*100/total_claims, 2)) + \"% of input\")\n",
        "  if start == 21000:\n",
        "    end = total_claims\n",
        "  else:\n",
        "    end = start + chunks\n",
        "\n",
        "  justifications = get_justifications(start, end)\n",
        "  j_inputs = list(map(process_sentence, justifications['sentence']))\n",
        "  j_targets = list(map(get_word_embeddings, list(map(get_sentence_subject, justifications['sentence']))))\n",
        "  j_chunks = torch.tensor(list(map(create_chunks, j_inputs)))\n",
        "\n",
        "  j_query, j_key, j_value = train_QKV(seralize_input(j_inputs), torch.stack(j_targets), j_chunks, j_query, j_key, j_value, j_epoches, j_learning_rate)\n",
        "\n",
        "  start += chunks\n",
        "  print(\"\")\n",
        "\n",
        "# turn off gradients\n",
        "j_query.requires_grad_(False)\n",
        "j_key.requires_grad_(False)\n",
        "j_value.requires_grad_(False)\n",
        "\n",
        "# save matrices\n",
        "torch.save(j_query, \"j_query.pt\")\n",
        "torch.save(j_key, \"j_key.pt\")\n",
        "torch.save(j_value, \"j_value.pt\")\n",
        "\n",
        "# to load them\n",
        "# j_query = torch.load(\"j_query.pt\")\n",
        "\n",
        "# download\n",
        "files.download(\"j_query.pt\")\n",
        "files.download(\"j_key.pt\")\n",
        "files.download(\"j_value.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "qbePMOvqkpDT",
        "outputId": "553e166c-197e-4c15-f53a-27e167272ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 0.0% of input\n",
            "Total Loss: 7.906126721700033\n",
            "\n",
            "Processing 0.95% of input\n",
            "Total Loss: 6.740224202473958\n",
            "\n",
            "Processing 1.89% of input\n",
            "Total Loss: 5.287273693084717\n",
            "\n",
            "Processing 2.84% of input\n",
            "Total Loss: 5.200796763102214\n",
            "\n",
            "Processing 3.78% of input\n",
            "Total Loss: 4.092413965861002\n",
            "\n",
            "Processing 4.73% of input\n",
            "Total Loss: 3.615651337305705\n",
            "\n",
            "Processing 5.67% of input\n",
            "Total Loss: 3.255082893371582\n",
            "\n",
            "Processing 6.62% of input\n",
            "Total Loss: 3.048530658086141\n",
            "\n",
            "Processing 7.57% of input\n",
            "Total Loss: 3.2482150077819822\n",
            "\n",
            "Processing 8.51% of input\n",
            "Total Loss: 2.554663864771525\n",
            "\n",
            "Processing 9.46% of input\n",
            "Total Loss: 3.190413554509481\n",
            "\n",
            "Processing 10.4% of input\n",
            "Total Loss: 3.095909754435221\n",
            "\n",
            "Processing 11.35% of input\n",
            "Total Loss: 3.3484044392903645\n",
            "\n",
            "Processing 12.3% of input\n",
            "Total Loss: 2.67353990872701\n",
            "\n",
            "Processing 13.24% of input\n",
            "Total Loss: 2.3339213530222573\n",
            "\n",
            "Processing 14.19% of input\n",
            "Total Loss: 2.6742533524831136\n",
            "\n",
            "Processing 15.13% of input\n",
            "Total Loss: 2.4170278390248616\n",
            "\n",
            "Processing 16.08% of input\n",
            "Total Loss: 2.2656555493672688\n",
            "\n",
            "Processing 17.02% of input\n",
            "Total Loss: 2.4824594497680663\n",
            "\n",
            "Processing 17.97% of input\n",
            "Total Loss: 2.092143408457438\n",
            "\n",
            "Processing 18.92% of input\n",
            "Total Loss: 1.8889072736104329\n",
            "\n",
            "Processing 19.86% of input\n",
            "Total Loss: 1.9507312297821044\n",
            "\n",
            "Processing 20.81% of input\n",
            "Total Loss: 2.0135816733042398\n",
            "\n",
            "Processing 21.75% of input\n",
            "Total Loss: 1.8151941696802776\n",
            "\n",
            "Processing 22.7% of input\n",
            "Total Loss: 1.5660574913024903\n",
            "\n",
            "Processing 23.65% of input\n",
            "Total Loss: 1.5448870499928793\n",
            "\n",
            "Processing 24.59% of input\n",
            "Total Loss: 1.4165279626846314\n",
            "\n",
            "Processing 25.54% of input\n",
            "Total Loss: 1.313829000790914\n",
            "\n",
            "Processing 26.48% of input\n",
            "Total Loss: 1.2323295593261718\n",
            "\n",
            "Processing 27.43% of input\n",
            "Total Loss: 1.1006857713063558\n",
            "\n",
            "Processing 28.37% of input\n",
            "Total Loss: 0.9619941393534343\n",
            "\n",
            "Processing 29.32% of input\n",
            "Total Loss: 0.8712255120277405\n",
            "\n",
            "Processing 30.27% of input\n",
            "Total Loss: 0.8484496593475341\n",
            "\n",
            "Processing 31.21% of input\n",
            "Total Loss: 0.7688939531644186\n",
            "\n",
            "Processing 32.16% of input\n",
            "Total Loss: 0.6476523598035177\n",
            "\n",
            "Processing 33.1% of input\n",
            "Total Loss: 0.5827945987383525\n",
            "\n",
            "Processing 34.05% of input\n",
            "Total Loss: 0.5340522845586141\n",
            "\n",
            "Processing 34.99% of input\n",
            "Total Loss: 0.5110187609990438\n",
            "\n",
            "Processing 35.94% of input\n",
            "Total Loss: 0.44815858999888103\n",
            "\n",
            "Processing 36.89% of input\n",
            "Total Loss: 0.41341493725776673\n",
            "\n",
            "Processing 37.83% of input\n",
            "Total Loss: 0.4104696889718374\n",
            "\n",
            "Processing 38.78% of input\n",
            "Total Loss: 0.38161237637201945\n",
            "\n",
            "Processing 39.72% of input\n",
            "Total Loss: 0.3717022160689036\n",
            "\n",
            "Processing 40.67% of input\n",
            "Total Loss: 0.37126080989837645\n",
            "\n",
            "Processing 41.62% of input\n",
            "Total Loss: 0.3978880882263184\n",
            "\n",
            "Processing 42.56% of input\n",
            "Total Loss: 0.36844833890597023\n",
            "\n",
            "Processing 43.51% of input\n",
            "Total Loss: 0.34097468852996826\n",
            "\n",
            "Processing 44.45% of input\n",
            "Total Loss: 0.3229782064755758\n",
            "\n",
            "Processing 45.4% of input\n",
            "Total Loss: 0.3632010360558828\n",
            "\n",
            "Processing 46.34% of input\n",
            "Total Loss: 0.3107802172501882\n",
            "\n",
            "Processing 47.29% of input\n",
            "Total Loss: 0.33170762459437053\n",
            "\n",
            "Processing 48.24% of input\n",
            "Total Loss: 0.3348865290482839\n",
            "\n",
            "Processing 49.18% of input\n",
            "Total Loss: 0.30879544019699096\n",
            "\n",
            "Processing 50.13% of input\n",
            "Total Loss: 0.3249348441759745\n",
            "\n",
            "Processing 51.07% of input\n",
            "Total Loss: 0.309221488237381\n",
            "\n",
            "Processing 52.02% of input\n",
            "Total Loss: 0.29707067807515464\n",
            "\n",
            "Processing 52.97% of input\n",
            "Total Loss: 0.3119069973627726\n",
            "\n",
            "Processing 53.91% of input\n",
            "Total Loss: 0.3007312377293905\n",
            "\n",
            "Processing 54.86% of input\n",
            "Total Loss: 0.3423734724521637\n",
            "\n",
            "Processing 55.8% of input\n",
            "Total Loss: 0.2994305074214935\n",
            "\n",
            "Processing 56.75% of input\n",
            "Total Loss: 0.31996384263038635\n",
            "\n",
            "Processing 57.69% of input\n",
            "Total Loss: 0.2968997935454051\n",
            "\n",
            "Processing 58.64% of input\n",
            "Total Loss: 0.29434401392936704\n",
            "\n",
            "Processing 59.59% of input\n",
            "Total Loss: 0.30779346426328025\n",
            "\n",
            "Processing 60.53% of input\n",
            "Total Loss: 0.3018631915251414\n",
            "\n",
            "Processing 61.48% of input\n",
            "Total Loss: 0.28988463083902993\n",
            "\n",
            "Processing 62.42% of input\n",
            "Total Loss: 0.2919698119163513\n",
            "\n",
            "Processing 63.37% of input\n",
            "Total Loss: 0.2908704976240794\n",
            "\n",
            "Processing 64.31% of input\n",
            "Total Loss: 0.2899690588315328\n",
            "\n",
            "Processing 65.26% of input\n",
            "Total Loss: 0.2920549670855204\n",
            "\n",
            "Processing 66.21% of input\n",
            "Total Loss: 0.2966119090716044\n",
            "\n",
            "Processing 67.15% of input\n",
            "Total Loss: 0.2809394141038259\n",
            "\n",
            "Processing 68.1% of input\n",
            "Total Loss: 0.2795917828877767\n",
            "\n",
            "Processing 69.04% of input\n",
            "Total Loss: 0.27254136006037394\n",
            "\n",
            "Processing 69.99% of input\n",
            "Total Loss: 0.27975805997848513\n",
            "\n",
            "Processing 70.94% of input\n",
            "Total Loss: 0.29660332600275674\n",
            "\n",
            "Processing 71.88% of input\n",
            "Total Loss: 0.2743445257345835\n",
            "\n",
            "Processing 72.83% of input\n",
            "Total Loss: 0.27764034469922383\n",
            "\n",
            "Processing 73.77% of input\n",
            "Total Loss: 0.2707242985566457\n",
            "\n",
            "Processing 74.72% of input\n",
            "Total Loss: 0.28284435669581093\n",
            "\n",
            "Processing 75.66% of input\n",
            "Total Loss: 0.27153420050938926\n",
            "\n",
            "Processing 76.61% of input\n",
            "Total Loss: 0.27178107301394144\n",
            "\n",
            "Processing 77.56% of input\n",
            "Total Loss: 0.26857593059539797\n",
            "\n",
            "Processing 78.5% of input\n",
            "Total Loss: 0.26709912419319154\n",
            "\n",
            "Processing 79.45% of input\n",
            "Total Loss: 0.284720903635025\n",
            "\n",
            "Processing 80.39% of input\n",
            "Total Loss: 0.26386532187461853\n",
            "\n",
            "Processing 81.34% of input\n",
            "Total Loss: 0.2824621816476186\n",
            "\n",
            "Processing 82.29% of input\n",
            "Total Loss: 0.2578403890132904\n",
            "\n",
            "Processing 83.23% of input\n",
            "Total Loss: 0.2733783841133118\n",
            "\n",
            "Processing 84.18% of input\n",
            "Total Loss: 0.2812869389851888\n",
            "\n",
            "Processing 85.12% of input\n",
            "Total Loss: 0.2811128516991933\n",
            "\n",
            "Processing 86.07% of input\n",
            "Total Loss: 0.31516286730766296\n",
            "\n",
            "Processing 87.01% of input\n",
            "Total Loss: 0.26654576261838275\n",
            "\n",
            "Processing 87.96% of input\n",
            "Total Loss: 0.29093484083811444\n",
            "\n",
            "Processing 88.91% of input\n",
            "Total Loss: 0.26306888461112976\n",
            "\n",
            "Processing 89.85% of input\n",
            "Total Loss: 0.26846290032068887\n",
            "\n",
            "Processing 90.8% of input\n",
            "Total Loss: 0.2551098724206289\n",
            "\n",
            "Processing 91.74% of input\n",
            "Total Loss: 0.2634211599826813\n",
            "\n",
            "Processing 92.69% of input\n",
            "Total Loss: 0.26091119249661765\n",
            "\n",
            "Processing 93.63% of input\n",
            "Total Loss: 0.27210105657577516\n",
            "\n",
            "Processing 94.58% of input\n",
            "Total Loss: 0.2753601173559825\n",
            "\n",
            "Processing 95.53% of input\n",
            "Total Loss: 0.25969504515329994\n",
            "\n",
            "Processing 96.47% of input\n",
            "Total Loss: 0.25964078307151794\n",
            "\n",
            "Processing 97.42% of input\n",
            "Total Loss: 0.25979265371958415\n",
            "\n",
            "Processing 98.36% of input\n",
            "Total Loss: 0.2650616943836212\n",
            "\n",
            "Processing 99.31% of input\n",
            "Total Loss: 0.25779778758684796\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a80d3018-630c-44ad-ac0a-658610515ce5\", \"j_query.pt\", 41180)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_684e23d7-2b36-485c-9070-045e98c7ab24\", \"j_key.pt\", 41106)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_357274b6-85b5-4b92-89ca-b266737f2058\", \"j_value.pt\", 41180)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train CJ Matrix"
      ],
      "metadata": {
        "id": "0HyNfE6DEL5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_CJ(input_data, output, CJ_Matrix, epoches = 20, learning_rate = 0.01, batch_size = 97):\n",
        "\n",
        "  assert input_data.size(1) == 200, \"Input data must have 200 features.\"\n",
        "  assert output.dim() == 1, \"Target data must be a 1D tensor.\"\n",
        "\n",
        "\n",
        "  class CJ_ANN(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(CJ_ANN, self).__init__()\n",
        "      self.layers = nn.Sequential(\n",
        "          nn.Linear(100, 64),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(64, 32),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(32, 8),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(8, 6)\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.layers(x)\n",
        "\n",
        "  model = CJ_ANN().to(device = dev)\n",
        "\n",
        "  # Loss and optimizer\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam([{'params': [CJ_Matrix]}, {'params': model.parameters()}], lr=learning_rate)\n",
        "\n",
        "  # Dataset and DataLoader\n",
        "  dataset = TensorDataset(input_data, output)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(epoches):\n",
        "    for inputs, targets in dataloader:\n",
        "      transformed_inputs = torch.matmul(CJ_Matrix, inputs.t()).t()\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = model(transformed_inputs)\n",
        "      loss = criterion(outputs, targets)\n",
        "\n",
        "      # Backward pass and optimization\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    # Print epoch info\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      print(f\"Epoch [{epoch + 1}/{epoches}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "  #return CJ_Matrix.detach(), model\n",
        "  return CJ_Matrix.detach()"
      ],
      "metadata": {
        "id": "y63Fmii9NIKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training CJ Matrix"
      ],
      "metadata": {
        "id": "fMV3Tg-CVSEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map Verdict Output\n",
        "\n",
        "map_verdict = {\"true\": 0, \"mostly-true\": 1, \"half-true\": 2, \"mostly-false\": 3, \"false\": 4, \"pants-fire\": 5}\n",
        "\n",
        "mapped_verdict = dataset[\"verdict\"].map(map_verdict)\n",
        "\n",
        "output_1 = torch.tensor(mapped_verdict.tolist(), device = dev, dtype=torch.long)\n",
        "output = output_1.repeat(5)"
      ],
      "metadata": {
        "id": "soj0h5zaQmxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_sentences(start, end):\n",
        "  C = []\n",
        "  J1 = []\n",
        "  J2 = []\n",
        "  J3 = []\n",
        "  J4 = []\n",
        "  J5 = []\n",
        "\n",
        "  for i in range(start, end):\n",
        "    file_path = f\"/content/top_justifications/top_justification_{i}.txt\"\n",
        "\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "      lines = file.readlines()\n",
        "\n",
        "    if len(lines) < 6:\n",
        "      sen_line = len(lines)\n",
        "\n",
        "      for k in range(6-sen_line):\n",
        "        lines.append(\"Deafult\")\n",
        "\n",
        "    C.append(lines[0].strip())\n",
        "    J1.append(lines[1].strip())\n",
        "    J2.append(lines[2].strip())\n",
        "    J3.append(lines[3].strip())\n",
        "    J4.append(lines[4].strip())\n",
        "    J5.append(lines[5].strip())\n",
        "\n",
        "  return C, J1, J2, J3, J4, J5"
      ],
      "metadata": {
        "id": "o-3V22AwtoAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_pre_CJ_matrix(sentence, query, key, value):\n",
        "\n",
        "  # get each word embeddings\n",
        "  processed_sentence = process_sentence(sentence)\n",
        "\n",
        "  # apply positional encodding\n",
        "  positional_encoding = get_positional_encodding(processed_sentence)\n",
        "\n",
        "  # Compute Q, K, V\n",
        "  Q = positional_encoding @ query\n",
        "  K = positional_encoding @ key\n",
        "  V = positional_encoding @ value\n",
        "\n",
        "  # Attention scores (Q.K^T) and softmax normalization\n",
        "  attention_scores = Q @ K.T\n",
        "  attention_weights = torch.softmax(attention_scores, dim=1)\n",
        "\n",
        "  # Weighted sum of values\n",
        "  output = attention_weights @ V\n",
        "\n",
        "  return add_and_normalize(output)"
      ],
      "metadata": {
        "id": "XgwtsLgj3iKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Input Data To Train CJ Matrix\n",
        "\n",
        "# Use All Trained Matrices\n",
        "c_query = torch.load(\"c_query.pt\")\n",
        "c_key = torch.load(\"c_key.pt\")\n",
        "c_value = torch.load(\"c_value.pt\")\n",
        "j_query = torch.load(\"j_query.pt\")\n",
        "j_key = torch.load(\"j_key.pt\")\n",
        "j_value = torch.load(\"j_value.pt\")\n",
        "\n",
        "# get all claims and justifications\n",
        "C, J1, J2, J3, J4, J5 = get_all_sentences(0, 21146)\n",
        "\n",
        "C = torch.stack([sentence_to_pre_CJ_matrix(sentence, c_query, c_key, c_value) for sentence in C])\n",
        "J1 = torch.stack([sentence_to_pre_CJ_matrix(sentence, j_query, j_key, j_value) for sentence in J1])\n",
        "J2 = torch.stack([sentence_to_pre_CJ_matrix(sentence, j_query, j_key, j_value) for sentence in J2])\n",
        "J3 = torch.stack([sentence_to_pre_CJ_matrix(sentence, j_query, j_key, j_value) for sentence in J3])\n",
        "J4 = torch.stack([sentence_to_pre_CJ_matrix(sentence, j_query, j_key, j_value) for sentence in J4])\n",
        "J5 = torch.stack([sentence_to_pre_CJ_matrix(sentence, j_query, j_key, j_value) for sentence in J5])\n"
      ],
      "metadata": {
        "id": "vuwyCVvgdk5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CJ1 = torch.zeros((21146, 200), device = dev)\n",
        "CJ2 = torch.zeros((21146, 200), device = dev)\n",
        "CJ3 = torch.zeros((21146, 200), device = dev)\n",
        "CJ4 = torch.zeros((21146, 200), device = dev)\n",
        "CJ5 = torch.zeros((21146, 200), device = dev)\n",
        "\n",
        "\n",
        "for i in range(21146):\n",
        "  CJ1[i] = combine(C[i], J1[i]).view(-1)\n",
        "\n",
        "for i in range(21146):\n",
        "  CJ2[i] = combine(C[i], J2[i]).view(-1)\n",
        "\n",
        "for i in range(21146):\n",
        "  CJ3[i] = combine(C[i], J3[i]).view(-1)\n",
        "\n",
        "for i in range(21146):\n",
        "  CJ4[i] = combine(C[i], J4[i]).view(-1)\n",
        "\n",
        "for i in range(21146):\n",
        "  CJ5[i] = combine(C[i], J5[i]).view(-1)\n",
        "\n",
        "\n",
        "\n",
        "# Append Vectors\n",
        "input_data = torch.cat((CJ1, CJ2, CJ3, CJ4, CJ5), device = dev, dim=0)"
      ],
      "metadata": {
        "id": "UAHr81tL-dAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CJ_Matrix = torch.randn(100, 200, device=dev, requires_grad=True)\n",
        "\n",
        "CJ_Matrix = train_CJ(input_data, output, CJ_Matrix, 30, 0.0005, 97)\n",
        "\n",
        "# turn off gradients\n",
        "CJ_Matrix.requires_grad_(False)\n",
        "\n",
        "# save matrices\n",
        "torch.save(CJ_Matrix, \"CJ_Matrix.pt\")\n",
        "\n",
        "# to load them\n",
        "# c_query = torch.load(\"CJ_Matrix.pt\")\n",
        "\n",
        "# to download\n",
        "files.download(\"CJ_Matrix.pt\")"
      ],
      "metadata": {
        "id": "YFMfVw_bETb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "collapsed": true,
        "outputId": "72c575b5-17ba-43a8-b5dc-00c7bbc918d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/30], Loss: 1.5807\n",
            "Epoch [20/30], Loss: 1.5320\n",
            "Epoch [30/30], Loss: 1.6771\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a6b4fb99-4868-4612-8def-6914444c163c\", \"CJ_Matrix.pt\", 81190)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Embeddings"
      ],
      "metadata": {
        "id": "5rC9NWpTVcrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_CJ_vector(CJ, CJ_Matrix):\n",
        "  return CJ_Matrix @ CJ"
      ],
      "metadata": {
        "id": "41wPjLcZg_Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load All Trained Matrices\n",
        "if torch.cuda.is_available():\n",
        "  c_query = torch.load(\"gpu_c_query.pt\")\n",
        "  c_key = torch.load(\"gpu_c_key.pt\")\n",
        "  c_value = torch.load(\"gpu_c_value.pt\")\n",
        "  j_query = torch.load(\"gpu_j_query.pt\")\n",
        "  j_key = torch.load(\"gpu_j_key.pt\")\n",
        "  j_value = torch.load(\"gpu_j_value.pt\")\n",
        "  CJ_Matrix = torch.load(\"gpu_CJ_Matrix.pt\")\n",
        "else:\n",
        "  c_query = torch.load(\"cpu_c_query.pt\")\n",
        "  c_key = torch.load(\"cpu_c_key.pt\")\n",
        "  c_value = torch.load(\"cpu_c_value.pt\")\n",
        "  j_query = torch.load(\"cpu_j_query.pt\")\n",
        "  j_key = torch.load(\"cpu_j_key.pt\")\n",
        "  j_value = torch.load(\"cpu_j_value.pt\")\n",
        "  CJ_Matrix = torch.load(\"cpu_CJ_Matrix.pt\")\n",
        "\n",
        "\n",
        "def KJNN_predict(C, J1, J2, J3, J4, J5):\n",
        "\n",
        "  # Preprocess Sentences\n",
        "  C = sentence_to_pre_CJ_matrix(C, c_query, c_key, c_value)\n",
        "  J1 = sentence_to_pre_CJ_matrix(J1, j_query, j_key, j_value)\n",
        "  J2 = sentence_to_pre_CJ_matrix(J2, j_query, j_key, j_value)\n",
        "  J3 = sentence_to_pre_CJ_matrix(J3, j_query, j_key, j_value)\n",
        "  J4 = sentence_to_pre_CJ_matrix(J4, j_query, j_key, j_value)\n",
        "  J5 = sentence_to_pre_CJ_matrix(J5, j_query, j_key, j_value)\n",
        "\n",
        "\n",
        "  # CJ Matrix\n",
        "  CJ1 = get_CJ_vector(combine(C, J1), CJ_Matrix)\n",
        "  CJ2 = get_CJ_vector(combine(C, J2), CJ_Matrix)\n",
        "  CJ3 = get_CJ_vector(combine(C, J3), CJ_Matrix)\n",
        "  CJ4 = get_CJ_vector(combine(C, J4), CJ_Matrix)\n",
        "  CJ5 = get_CJ_vector(combine(C, J5), CJ_Matrix)\n",
        "\n",
        "  # Final Add and Normalize\n",
        "  return add_and_normalize(torch.stack([CJ1, CJ2, CJ3, CJ4, CJ5], dim = 0).squeeze(2))"
      ],
      "metadata": {
        "id": "Agmdy0FsVhFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5Ec4gZ5jNPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C = \"John McCain opposed bankruptcy protections for families 'who were only in bankruptcy because of medical expenses they couldn't pay.'\"\n",
        "J1 = \"specifically he noted mccain opposition to an effort to exempt from the law individuals whose medical expenses pushed them into bankruptcy\"\n",
        "J2 = \"john mccain support for a law that made it more difficult for personal bankruptcy filers to escape debts that they could repay\"\n",
        "J3 = \"and when i fought against the credit card industry bankruptcy bill that made it harder for working families to climb out of debt he supported it and he even opposed exempting families who were only in bankruptcy because of medical expenses they could pay\"\n",
        "J4 = \"when he had the chance to help families avoid falling into debt john mccain sided with the credit card companies\"\n",
        "J5 = \"because obama correctly cites mccain vote on an effort to narrow the bankruptcy law reach we judge his statement true\""
      ],
      "metadata": {
        "id": "JOoRW7t1ERuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = KJNN_predict(C, J1, J2, J3, J4, J5)"
      ],
      "metadata": {
        "id": "tWdxL49rERqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286b03de-a5b8-42ca-e1aa-0d182472d5e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-1fc095de380b>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  c_query = torch.load(\"cpu_c_query.pt\")\n",
            "<ipython-input-51-1fc095de380b>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  c_key = torch.load(\"cpu_c_key.pt\")\n",
            "<ipython-input-51-1fc095de380b>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  c_value = torch.load(\"cpu_c_value.pt\")\n",
            "<ipython-input-51-1fc095de380b>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  j_query = torch.load(\"cpu_j_query.pt\")\n",
            "<ipython-input-51-1fc095de380b>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  j_key = torch.load(\"cpu_j_key.pt\")\n",
            "<ipython-input-51-1fc095de380b>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  j_value = torch.load(\"cpu_j_value.pt\")\n",
            "<ipython-input-51-1fc095de380b>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  CJ_Matrix = torch.load(\"cpu_CJ_Matrix.pt\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEBCwpoZqBV5",
        "outputId": "7dd432a5-095a-4c3f-a2a8-bdac3aa72b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0694, -0.0814,  0.0196, -0.0171,  0.0569, -0.0351, -0.0316, -0.0321,\n",
            "        -0.0317, -0.0199,  0.0590,  0.0939,  0.0269, -0.0309, -0.0903,  0.1504,\n",
            "         0.1229,  0.0107, -0.0748,  0.0362,  0.0613, -0.1196, -0.0671,  0.2501,\n",
            "         0.1448, -0.0598, -0.0166,  0.1190, -0.0941,  0.0018, -0.0463,  0.0606,\n",
            "        -0.0269,  0.1142, -0.1825,  0.0949,  0.0731, -0.0311, -0.0330, -0.0211,\n",
            "        -0.0710, -0.1926, -0.0364, -0.1535,  0.2103, -0.2964,  0.0693,  0.1662,\n",
            "         0.0831, -0.0082,  0.0905,  0.2361, -0.0831,  0.1077,  0.0582, -0.0155,\n",
            "         0.0147,  0.0782,  0.1124, -0.2363,  0.0536,  0.0171,  0.1002, -0.0417,\n",
            "         0.0057,  0.0397, -0.0136,  0.0536, -0.0118,  0.0888,  0.0483,  0.0480,\n",
            "        -0.0637, -0.0036, -0.0574,  0.1713,  0.1615,  0.0436, -0.0747,  0.0598,\n",
            "         0.0126,  0.0404, -0.0334,  0.1542,  0.0343,  0.1257, -0.0526, -0.0629,\n",
            "        -0.0883, -0.0385, -0.0493,  0.0894, -0.0597,  0.0111, -0.0630, -0.0748,\n",
            "        -0.0251, -0.1807, -0.2768,  0.1080])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(vector))"
      ],
      "metadata": {
        "id": "WQGcQtvkERob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bb8faac-3d21-4cf9-ee32-8da4a108954a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector.shape)"
      ],
      "metadata": {
        "id": "yogZr2-rERmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fa93721-aa08-43c2-d5f6-7b005f6462ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100])\n"
          ]
        }
      ]
    }
  ]
}